{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e86c65a8-e130-430c-b271-0e5cbfad4d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output after training:\n",
      "[[0.00260572]\n",
      " [0.99672209]\n",
      " [0.99701711]\n",
      " [0.00386759]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# === Input Data ===\n",
    "# Four training examples, each with 3 binary input features (like pixels or binary signals)\n",
    "X = np.array([[0, 0, 1],\n",
    "              [0, 1, 1],\n",
    "              [1, 0, 1],\n",
    "              [1, 1, 1]])\n",
    "\n",
    "# Output labels (supervised learning): expected outputs for each input\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# Set seed to make random numbers reproducible\n",
    "np.random.seed(1)\n",
    "\n",
    "# Weight Initialization\n",
    "# syn0: weights connecting Input layer (3 neurons) to Hidden layer (4 neurons)\n",
    "syn0 = 2 * np.random.random((3, 4)) - 1\n",
    "\n",
    "# syn1: weights connecting Hidden layer (4 neurons) to Output layer (1 neuron)\n",
    "syn1 = 2 * np.random.random((4, 1)) - 1\n",
    "\n",
    "# Training Loop \n",
    "# We’ll train the network for 60,000 iterations to let it learn from the data\n",
    "for j in range(60000):\n",
    "\n",
    "    # FORWARD PROPAGATION \n",
    "    # Step 1: Input → Hidden layer\n",
    "    # We calculate the dot product of inputs and syn0 (weights), then apply the sigmoid activation function\n",
    "    l1 = 1 / (1 + np.exp(-np.dot(X, syn0)))  # l1 is the output of the hidden layer\n",
    "\n",
    "    # Step 2: Hidden → Output layer\n",
    "    # Again, dot product with weights (syn1) and apply sigmoid to get prediction\n",
    "    l2 = 1 / (1 + np.exp(-np.dot(l1, syn1)))  # l2 is the predicted output of the network\n",
    "\n",
    "    # BACKPROPAGATION\n",
    "    # Step 3: Calculate error in prediction\n",
    "    l2_error = y - l2  # How much we missed the target\n",
    "\n",
    "    # Step 4: Calculate gradient at the output layer\n",
    "    # Derivative of sigmoid: l2 * (1 - l2) tells us how confident the neuron is\n",
    "    l2_delta = l2_error * l2 * (1 - l2)\n",
    "\n",
    "    # Step 5: Backpropagate the error to the hidden layer\n",
    "    l1_error = l2_delta.dot(syn1.T)  # Error contributed by each hidden neuron\n",
    "    l1_delta = l1_error * l1 * (1 - l1)  # Multiply by sigmoid gradient for hidden layer\n",
    "\n",
    "    # WEIGHT UPDATES \n",
    "    # Adjust weights by how much they contributed to the error, scaled by the input\n",
    "    syn1 += l1.T.dot(l2_delta)  # Update syn1 (hidden to output)\n",
    "    syn0 += X.T.dot(l1_delta)   # Update syn0 (input to hidden)\n",
    "\n",
    "# === Final Output After Training ===\n",
    "print(\"Output after training:\")\n",
    "print(l2)  # This should be close to y = [[0], [1], [1], [0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76eb8b30-0aa9-44e2-8fc0-bd6224c82981",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96986bb7-c3d8-4ff6-8d5d-5f4473766d22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
